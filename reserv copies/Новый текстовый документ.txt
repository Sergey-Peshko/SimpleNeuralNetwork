void neuralNet::BackpropagationLearningAlgorithm::train(IMultilayerNeuralNetwork * network, vector<DataItem<float>*>* data)
	{
		if (_config->getBatchSize() < 1 || _config->getBatchSize() > data->size())
		{
			_config->setBatchSize(data->size());
		}
		float currentError = FLT_MAX;
		float lastError = 0;
		int epochNumber = 0;
		_logger << ("Start learning...") << std::endl;

		//#region initialize accumulated error for batch, for weights and biases

		float*** nablaWeights = new float**[network->getLayers()->size()];
		float** nablaThresholds = new float*[network->getLayers()->size()];

		for (int i = 0; i < network->getLayers()->size(); i++)
		{
			nablaWeights[i] = new float*[network->getLayers()->at(i)->getNeurons()->size()];
			nablaThresholds[i] = new float[network->getLayers()->at(i)->getNeurons()->size()];
			for (int j = 0; j < network->getLayers()->at(i)->getNeurons()->size(); j++)
			{
				nablaWeights[i][j] = new float[network->getLayers()->at(i)->getNeurons()->at(j)->getWeights()->size()];
			}
		}

		//#endregion

		do
		{
			lastError = currentError;
			int dtStart = clock();

			//preparation for epoche
			vector<int>* trainingIndices = new vector<int>(data->size());
			for (int i = 0; i < data->size(); i++)
			{
				trainingIndices->at(i) = i;
			}
			if (_config->getBatchSize() > 0)
			{
				shuffle(trainingIndices);
			}

			//process data set
			int currentIndex = 0;
			do
			{
				//обнуление ошибок группы
				for (int i = 0; i < network->getLayers()->size(); i++)
				{
					for (int j = 0; j < network->getLayers()->at(i)->getNeurons()->size(); j++)
					{
						for (int k = 0; k < network->getLayers()->at(i)->getNeurons()->at(j)->getWeights()->size(); k++)
						{
							nablaWeights[i][j][k] = 0;
						}
						nablaThresholds[i][j] = 0;
					}
				}

					//process one batch
					for (int inBatchIndex = currentIndex; inBatchIndex < currentIndex + _config->getBatchSize() && inBatchIndex < data->size(); inBatchIndex++)
					{
						//forward pass
						vector<float>* realOutput = network->calculateOutput(data->at(trainingIndices->at(inBatchIndex))->getInput());


						//backward pass, error propagation
						//last layer
						//.......................................ОБРАБОТКА ПОСЛЕДНЕГО СЛОЯ
						for (int j = 0; j < network->getLayers()->at(network->getLayers()->size() - 1)->getNeurons()->size(); j++)
						{
							network->getLayers()->at(network->getLayers()->size() - 1)->getNeurons()->at(j)->setLastError(
								_config->getErrorFunction()->calculatePartialDerivaitve(
									data->at(inBatchIndex)->getOutput(),
									realOutput, j) *
								network->getLayers()->at(network->getLayers()->size() - 1)->getNeurons()->at(j)->getActivationFunction()->
								calculateFirstDerivative(network->getLayers()->at(network->getLayers()->size() - 1)->getNeurons()->at(j)->getLastSum()));

							nablaBiases[network.Layers.Length - 1][j] += _config.LearningRate *
								network.Layers[network.Layers.Length - 1].Neurons[j].dEdz;

							for (int i = 0; i < network.Layers[network.Layers.Length - 1].Neurons[j].Weights.Length; i++)
							{
								nablaWeights[network.Layers.Length - 1][j][i] +=
									_config.LearningRate*(network.Layers[network.Layers.Length - 1].Neurons[j].dEdz*
									(network.Layers.Length > 1 ?
										network.Layers[network.Layers.Length - 1 - 1].Neurons[i].LastState :
										data[inBatchIndex].Input[i])
										+
										_config.RegularizationFactor *
										network.Layers[network.Layers.Length - 1].Neurons[j].Weights[i]
										/ data.Count);
							}
						}


						//hidden layers
						//.......................................ОБРАБОТКА СКРЫТЫХ СЛОЕВ
						for (int hiddenLayerIndex = network.Layers.Length - 2; hiddenLayerIndex >= 0; hiddenLayerIndex--)
						{
							for (int j = 0; j < network.Layers[hiddenLayerIndex].Neurons.Length; j++)
							{
								network.Layers[hiddenLayerIndex].Neurons[j].dEdz = 0;
								for (int k = 0; k < network.Layers[hiddenLayerIndex + 1].Neurons.Length; k++)
								{
									network.Layers[hiddenLayerIndex].Neurons[j].dEdz +=
										network.Layers[hiddenLayerIndex + 1].Neurons[k].Weights[j] *
										network.Layers[hiddenLayerIndex + 1].Neurons[k].dEdz;
								}
								network.Layers[hiddenLayerIndex].Neurons[j].dEdz *=
									network.Layers[hiddenLayerIndex].Neurons[j].ActivationFunction.
									ComputeFirstDerivative(
										network.Layers[hiddenLayerIndex].Neurons[j].LastNET
									);

								nablaBiases[hiddenLayerIndex][j] += _config.LearningRate*
									network.Layers[hiddenLayerIndex].Neurons[j].dEdz;

								for (int i = 0; i < network.Layers[hiddenLayerIndex].Neurons[j].Weights.Length; i++)
								{
									nablaWeights[hiddenLayerIndex][j][i] += _config.LearningRate * (
										network.Layers[hiddenLayerIndex].Neurons[j].dEdz *
										(hiddenLayerIndex > 0 ? network.Layers[hiddenLayerIndex - 1].Neurons[i].LastState : data[inBatchIndex].Input[i])
										+
										_config.RegularizationFactor * network.Layers[hiddenLayerIndex].Neurons[j].Weights[i] / data.Count
										);
								}
							}
						}

						//чистка
						delete realOutput;
					}

				//update weights and bias
				for (int layerIndex = 0; layerIndex < network->getLayers()->size(); layerIndex++)
				{
					for (int neuronIndex = 0; 
						neuronIndex < network->getLayers()->at(layerIndex)->getNeurons()->size(); 
						neuronIndex++)
					{
						network->getLayers()->at(layerIndex)->getNeurons()->at(neuronIndex)->setThreshold(
							network->getLayers()->at(layerIndex)->getNeurons()->at(neuronIndex)->getThreshold()
							- nablaThresholds[layerIndex][neuronIndex]);
						for (int weightIndex = 0; 
							weightIndex < network->getLayers()->at(layerIndex)->getNeurons()->at(neuronIndex)->getWeights()->size();
							weightIndex++)
						{
							network->getLayers()->at(layerIndex)->getNeurons()->at(neuronIndex)->getWeights()->at(weightIndex) -=
								nablaWeights[layerIndex][neuronIndex][weightIndex];
						}
					}
				}

				currentIndex += _config->getBatchSize();

			} while (currentIndex < data->size());

			//recalculating error on all data
			//real error
			currentError = 0;
			for (int i = 0; i < data.Count; i++)
			{
				double[] realOutput = network.ComputeOutput(data[i].Input);
				currentError += _config.ErrorFunction.Calculate(data[i].Output, realOutput);
			}
			currentError *= 1d / data.Count;
			//regularization term
			if (Math.Abs(_config.RegularizationFactor - 0d) > Double.Epsilon)
			{
				double reg = 0;
				for (int layerIndex = 0; layerIndex < network.Layers.Length; layerIndex++)
				{
					for (int neuronIndex = 0; neuronIndex < network.Layers[layerIndex].Neurons.Length; neuronIndex++)
					{
						for (int weightIndex = 0; weightIndex < network.Layers[layerIndex].Neurons[neuronIndex].Weights.Length; weightIndex++)
						{
							reg += network.Layers[layerIndex].Neurons[neuronIndex].Weights[weightIndex] *
								network.Layers[layerIndex].Neurons[neuronIndex].Weights[weightIndex];
						}
					}
				}
				currentError += _config.RegularizationFactor * reg / (2 * data.Count);
			}

			epochNumber++;
			Logger.Instance.Log("Eposh #" + epochNumber.ToString() +
				" finished; current error is " + currentError.ToString() +
				"; it takes: " +
				(DateTime.Now - dtStart).Duration().ToString());
			//чистка
			delete trainingIndices;
		} while (epochNumber < _config->getMaxEpoches() &&
			currentError > _config->getMinError() &&
			abs(currentError - lastError) > _config->getMinErrorChange());

		//чистка памяти nablов
		for (int i = 0; i < network->getLayers()->size(); i++)
		{
			for (int j = 0; j < network->getLayers()->at(i)->getNeurons()->size(); j++)
			{
				delete nablaWeights[i][j];
			}
			delete nablaWeights[i];
			delete nablaThresholds[i];
		}
		delete nablaWeights;
		delete nablaThresholds;
	}